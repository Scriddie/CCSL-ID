\documentclass{article}

\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[at]{easylist}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{pdfpages}
\usepackage{cleveref}
\usepackage{todonotes}
\usepackage[
    doi=false,isbn=false,url=false,
    backend=biber,
    uniquename=false,
    bibencoding=utf8,
    sorting=nty,
    citestyle=authoryear,
    ]{biblatex}
    \addbibresource{bibliography.bib}

\title{Reading Notes}
\author{Alexander Reisach}

\begin{document}
    
\maketitle

\section{Plan}
\begin{easylist}
    \ListProperties(Style1*=\bfseries,Numbers2=l,Mark1={},Mark2={)},Indent2=1em)
    @ Start by looking at data generation methods
    @@ Can I find the \cite{sachs2005causal} pathways in \cite{belinky2015pathcards} and \cite{perfetto2016signor}?
    @@ Do \cite{van2006syntren} and \cite{schaffter2011genenetweaver} produce varsortable data?
    @ Take a look at real-world data
    @ Implement \cite{brouillard2020differentiable}
    @ Consider transfer learning regret
    @ Try cycle breaking
\end{easylist}

\section{Related Literature}
Nice resource on cell signalling pathways: \url{https://www.khanacademy.org/science/ap-biology/cell-communication-and-cell-cycle/changes-in-signal-transduction-pathways/a/intracellular-signal-transduction}.

\subsection{Causal Structure Learning}
\cite{itani2010structure} provide a good recipe for learning cyclical causal structure from interventional data.
\cite{vowels2021} give a comprehensive overview over all kinds of structure learning methods and benchmarks.
\cite{van2006syntren} provide a good overview over other network simulations.

\subsubsection{DCDI}
\noindent
TODOs
\begin{enumerate}
    \item Shouldnt they have to copy x at some point in their loss calculation?
    \item What exactly are their "conditional distributions"?
    \item Check out their "plot-density" for the two-variable case!
    \item How does their obs data solution compare to lars lasso bic regression with random direction? 
    \item How does this relate to the interventional case?
    \item Can I create a "counterexample" with some special interventions?
    \item What value do obs data give us? Is there an optimal ratio obs/int 
    data?
    \item What's the impact of interventions having a different distribution?
    \item don't a lot of things just depend on our number of (obs/int) data points? (which will determine our likelihood)
    \item What are these extra params?
    \item Check the different gumbel matrices
    \item We should try treating perfect inter as unknown but with identical params (mask out at input stage)
    \item Their method (with standardization per regime) should not work for non-linear relationships
    \item Where do the spikes come from in the loss chart? Are we losing the NLL-params at every aug. lagrangian update step?
    \item We'd have to have a network with optimal parameters ready for every possible combinations of inputs? Since that's not feasible, can the networks retrain sensibly?
    \item TODO I should look at that paper that claims to have 70\% success on the tuebingen cause-effect data set.
\end{enumerate}

\noindent
Observations
\begin{enumerate}
    \item They are scaling each interventional distribution separately right after generation (dat\_generator.py). This might lose a lot of information? In their default settings they use uniform, so it is less severe. But for shift\_normal, it counteracts some of the purpose of the intervention.
    \item In their data they seem to have standardized per regime
    \item They also standardize the entire dataset later, but those parts of the code are bugged and do not work (generate\_data.py)!
    \item Getting the combination of lr and lagrangian penalties right is not straightforward. We need to keep the mu increases small.
    \item In their implementation, they effectively model distributions first, then slowly let the lagrangian kick in
    \item They do not solve the observational case particularly well (they get the causal direction in Gaussian NV ANMs 50-50, just as one would expect and like notears.)
    \item Outlier removal might make a difference, at least in the MEK-ERK case.
    \item MEK-ERK works; PKA-P38 fails; PKA-JNK works; PKA-ERK works;
    \item It seems that too large a body of obs data can render the int data ineffective
    \item Stochasticity at small batch size can be a problem
    \item What are they doing with name\_data??? It's never read at all? Currently we are applying intervention masks to observational data?
    \item Their sparsity constraint hyperparam can really mess things up. Is there a good way of setting it? By setting it to zero, I can learn something sensible in the bivariate case
    \item For MEK the outliers cause a completely different distributin after scaling in the obs setting
    \item Whether or not to standardize regimes individually is not that straightforward!
    \item U0126 perturbation might be a special case as it does not prevent phosphorylation but inhibits otherwise from what I understand. We see this across charts, e.g. in RAF-MEK - seems like a lot more MEK is produced under inhibitor. Interestingly, under U0126, there is a clear trend between PKC and JNK that's otherwise absent.
    \item Their compute\_loss nll still doesn't seem right. Shouldn't they divide by the number of non-zero mask entries?
    \item Distribution fits do not get worse because mask is applied only at loss calculation.
    \item If I am right about the potential but in nll calculation, then I should see that an intervention on the effect inevitably leats to learning the effect as a cause!
    \item Why is their code working even if I intervene on effect???
    \item Sometimes their nlls are negative. Can that be a problem? logsumexp?
    \item Maybe I should standardize each intervention separately??? the shift from zero might be why we learn the correct causal direction, even with an intervention on the effect node!
    \item We have to scale each regime individually for likelihood, but doesn't that also loose some (a lot of) information? is there a logical contradiction?
    \item For the two variables I picked, which have a pretty clear linear trend, if I mask out 50\% such that they can only be used for explanations one way - it should kind of work. I wonder if  there is a way to connect this back to the bengio transfer-learning paper. After all DCDI also chooses the model that minimizes conditional nll. If the optimal likelihoods X->Y and Y<-X are quite similar, perhaps the intervention will make the difference. If they are quite different, we might need a very strong intervention to overcome that inherent bias...
    \item Seems like whether or not the trend in int and obs is the same makes quite a difference (at least when the support overlaps?). This might be related to the obs/marg likelihoods.
    \item I'm still not quite sure about the standardization procedures in that regard. I've been thinking that one should standardize the entire data set rather than each regime. That would allow to take advantage of potential increases in support through intervantions. On the flip sice, in the case of a shift intervention, it really compresses both peaks which has an impact on the likelihood.
    In DCDI's initial implementation where they standardize each regime separately, that is not a problem. However, they may end up with having the same support for observational and interventional data and will require the optimal weight to be identical, even if it was rightfully different on the raw scale.
    \item extra params model log stds (learnable but do not depend on parents) That means distribution stds do not depend on parents?
    \item If not one of two opposing edges goes to zero immediately, they will 'fight' and both end up at zero. There needs to be a convincing winner. The problem could potentially be mitigated by choosing smaller mu update steps in the lagrangian.
    \item The acyclicity constraint may eliminate the inputs to cycles before the actual cycles. This could be a fundamental problem!
    \item I am still very unsure about the individual standardization
    \item They assume homoscedasticity
    \item The adversarial sachs et al example should also be bad news for sparsity constraints, no?
    \item Under model misspecification I have no score equivalence either way. Why no take score equivalence to make sure I specified my model correcty?
    \item The gumbel initialization makes a massive difference! With low starting values, even good fits achieve next to nothing
    \item change greater to greater equal in "delta\_gamma >= 0" to prevent getting stuck on plateaus
    \item Learning rate is quite important, decides whether we race edges against each other or just send certain ones to zero almost instantly
    \item Remove zombie edges from acyclicity violation, too. They can just drag down good edges when penalty goes high very quickly
    \item The fact that the net is trying to fit the zeros induced by the gumbel adjacency looks to be a real problem, especially in an edge race. Maybe we should just predict zero in these cases without using the network? For multiple inputs, if some were masked out, we should indicate somehow that we didn't have this value (not the same as zero)?
    \item Confirmed: Even without any cycles, we can lose perfectly good edges if they just happen to have a rather low value. Is this because missingness isn't indicated??
    \item Missingness indicator seems nice, but somehow we can't seem to fit the sinus curve well anymore. Missingness indicator needs some more debugging...
    \item Seems like we have a bit of a different case where it's not the intervention that's imperfect but rather the intervention changing the causal relationship!
    \item For x**2, we see the good edge falling even after it's opposite is gone! Is this because here we have a mismatch that we want to map zero to very different things??
    \item We definitely need to start with high values, otherwise the frequent missingness fucks up the fit too much. Seems like a max value even higher than the start value also helps :). 
    \item Once an edge goes below a certain threshold, the zero abiguity just throws it off. Is that remedied by individual scaling? 
    \item Finding the right scale for the acyclicity violation is very hard! just setting some value might result in cycles in large graphs with high dag_normalization...
\end{enumerate}

\subsection{Which interventions to choose from}
in \cite{} and \cite{} they don't use the activation interventions because they argue they would affect receptor enzymes instead of measured signaling proteins. These interventions don't (de-)phosporylate anything and therefore don't change the abundance of the protein. Yet they do change the activity.

In fact, in \url{https://en.wikipedia.org/wiki/Protein_kinase_A#:~:text=In%20direct%20protein%20phosphorylation%2C%20PKA,the%20synthesis%20of%20the%20protein} (section Activation, bottom) it looks to me like cAMP would indeed result in an abundance increase that can be measured by the PKA antibody used in the Sachs et al. data set. This would mean we can use the PKA intervention (after all this part works best of all!).

Question: Does the PKA antibody actually measure PKA, or does it measure it's effects???

How should we best make use of these interventions? Investigate if there is a change in abundancy. If there is absolutely none, does our scaling help at all?
Using a different network for each interventional distribution could help us cope with that. A separate network for each range would make it harder to argue for an invariant causal effect, but it will allow for a type of activation that does not consist in the change of abundance.

\subsection{Score}

% \begin{equation}
    
% \end{equation}

\subsubsection{General Model}
\begin{equation}
    f^{(k)}(X, M, R, \phi) = \prod_{j=1}^{d} \tilde{f}(x_j, NN(M_j \odot x, \phi^{(1)}))^{1-R_{kj}} \tilde{f}(x_j, NN(M_j \odot x, \phi^{(k)}))^{R_{kj}}
\end{equation}

\begin{equation}
    S_{\mathcal{I}}(G) = \sup \sum_{k=1}^K \mathbb{E}_{X ~ p^{k}} \log f^{(k)}(X, M, R, \phi) - \lambda |G|
\end{equation}

% TODO: check if mean is in right place
\begin{equation}
    \underset{G}{\mathrm{argmax}} \quad \sum_{k=1}^K \frac{1}{N} \sum_{i=1}^N \log(\tilde{f}(x_j, NN(M_j \odot x, \phi^{(1)}))^{1-R_{kj}} \tilde{f}(x_j, NN(M_j \odot x, \phi^{(k)}))^{R_{kj}}) - \lambda |G|
\end{equation}

\subsubsection{Perfect Interventions}
% Score for perfect interventions
\begin{equation}
    \begin{split}
        & X \in \mathbb{R}^{n \times d} \\
        & M^G \in \mathbb{R}^{d \times d} := \text{sigmoid}(G) > 0.5 \\
        & \mathcal{M} \in \mathbb{R}^{n \times d} \quad \text{(intervention mask)} \\
        & \underset{G}{\mathrm{argmax}} \quad \sum^n_{i=1} \sum^d_{j=1} \mathcal{N}(\text{NN}(M^G_j \odot x^{(i)}, \phi), 1)_{\text{logPDF}}(x^{(i)}_j) \cdot \mathcal{M}^{(i)}_j
    \end{split}
\end{equation}


\subsection{Pseudocode}
\begin{verbatim}
    for i in range(num_vars):
        
        if intervention == 'perfect':
            density_params = model(x, weights, biases, extra_params)
            log_probs = distribution(density_params).log_prob(x[:, i]) * mask

        else:  # imperfect/unknown intervention
            density_params = model(x, weights, biases, extra_params, masks, regime)
            log_probs = distribution(density_params).log_prob(x[:. i])
        
        log_likelihood = torch.mean(log_probs)

\end{verbatim}


\subsection{Causal Networks in Biology}
The original paper on the dream challenges etc. is \cite{hill2016inferring}.
DREAM challenge data should be available here \url{https://www.synapse.org/#!Synapse:syn1720047}.
\cite{hill2017context} address the problem of benchmarking causal network inference in biological data. Seems like they just recycled some of their findings from their 2016 paper? They say: However, we note that empirical
assessment is a frontier topic in causal inference, and the
assessment procedure used here is subject to a number of ca-
veats.
The \emph{in silico} data used in the DREAM challenges was generated according to \cite{chen2009input}.
They are using some sort of funny score and don't seem to trust it a lot themselves. Previous DREAM challenges were predictive rather than causal.
For the assessment they use some unseen interventions to get a gold standard network and du AUROC on that.

\paragraph{Info on DREAM data}
Basic information on the dataset \url{https://www.synapse.org/#!Wiki:syn1720047/ENTITY/56061}. Additional information on the dataset\url{https://www.synapse.org/#!Wiki:syn1720047/ENTITY/56210}. The webpage for the 2013 DREAM challenge can be found here \url{https://dreamchallenges.org/dream-8-hpn-dream-breast-cancer-network-inference-challenge/}.

Nice overview on cell network inference here \url{http://compbio.mit.edu/marbach/projectsdaniel.html}.

This 5-node network in yeast might also be useful \cite{cantone2009yeast}.

\vspace{.5em}

It looks like this guz\url{https://github.com/gungorbudak/netinf-bigcat} did a pretty similar thing at bigcat in Maastricht 8 years ago?
This MSc thesis is also very similar in spirit \url{https://github.com/ninakersten/Masterthesis}.

\subsection{Datasets}

I am not sure there are much better data sets than the one by \cite{sachs2005causal}.
In the paper they talk about some well-known connections. Take those and look at them!
Several of the known connections from
our model are direct enzyme-substrate rela-
tionships (Fig. 3B) (PKA to Raf, Raf to Mek,
Mek to Erk, and Plc-g to PIP 2 ),

\paragraph{SynTReN}
is a synthetic TRN data generation scheme proposed by \cite{van2006syntren}. 
They talk about ODE simulations. Should have a look at them.
Check out the original source networks they consider, also look for more up-to-date networks
They seem to just do additive noise models
Visualize and compare that to Sachs et al and another dataset from vowels2021

\paragraph{GeneNetWeaver} is a synthetic TRN data generation scheme proposed by \cite{schaffter2011genenetweaver}. Their model was used in benchmarking the Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenges.

\paragraph{SIGNOR} is a database of established causal relationships between biological entities \cite{perfetto2016signor}.

\paragraph{PathCards} unifies several sources on human biological pathways \cite{belinky2015pathcards}.
They provide a good overview over other pathway sources.

\paragraph{OmniPath} I should probably use OmniPath to make sure the neetworks used in the DREAM challenge are still up to date?

\subsection{Benchmarking}

Ernest Fraenkel\footnote{\url{https://www.youtube.com/watch?v=RBPcKbEvK3U&list=PLUl4u3cNGP63uK-oWiLgO7LLJV6ZCWXac&index=17}} gives a very nice overview over \cite{marbach2012wisdom} where he explains how some funamental assumptions may be violated in real-world cell signalling data.
He shows that unlike in in-silico data and E.coli data, for yeast co-regulated genes are basically uncorrelated and have no mutual information.

Tuebingen causality pairs \url{https://webdav.tuebingen.mpg.de/cause-effect/} (only observational).
Try varsortability in the bivariate case?


\section{Conceptional Work}
    - Leap of abstraction from PDE to causal graph

\section{Methodology}
\begin{itemize}
    \item Only apply acyclicity constraint gradually/at a later stage?
    \item Do not optimize over diagonal?
    \item Use dropout (predictions/gradients)?
    \item Combine continuous structure learning with cyclic relationships
\end{itemize}

\subsection{Transfer Learning Regret}
Do \cite{bengio2019meta} normalize? if not, isn't maybe the variance in one direction just bigger than in the other which requires more steps? In the high-variance direction, we might see bigger losses at the start $\dots$
How do they generate their data?

In Bengio's paper, are we ultimately just talking about inverting a (mathematically) irreversible function?
Isn't that conceptionally exactly the same as the light switch example where X inevitably causes Y but Y can also be caused by many other things?

Is there something to be gained from viewing data as a sequence in the style of \cite{bengio2019meta}? Mybe try transfer learning regret on bio data?

Is it maybe enough to give the conditional more params to fit than the marginal? :O
Actually, can we just test which way the marginal changes more? The conditional will have to account for the rest of the variation...

Is there a way to break causality by including the wrong things? Not just by treating them in the wrong way, but by including them in the question at all!

\subsection{Experiments}
\paragraph{Current theory}
Whichever model has the higher nll, that's the model the meta-learner will move towards?
Alternatively, maybe meta learning will work once we have output noise??
We do have a lot of random fluctuations either way...
With output noise, the conditional loss of y2x seems to be a lot lower. Not anymore without output noise?
\begin{itemize}
    \item TODO maybe there is catastrophic forgetting in only one direction? say, if in one direction it would be easier to re-fit the already fitted components / refitting would require some lasting change!
    \item Does standardization remove impact of scale range?
    \item output noise has an effect on the relative data scales! With sufficiently small additive noise, we can get alpha to go either way
    \item Is it about which part of the network would have to 'move the farthest'??
    \item The network learns combinations that work fine for the input range but go bananas beyound, even for simple stuff like all zeros, or abline targets
    \item One could look at 'samples until convergence', rather than total regret, that would effectively check how much params have to change.
    \item I am still not quite sure if its a bias of their model or something about the data generation
\end{itemize}

\clearpage
\subsection{Findings}
\begin{itemize}
    \item We need to add some significant noise to at least one variable, otherwise we risk collapsing probabilities
    \item Evaluation data set needs to be large for consistent effects
    \item For equal additive noise on both variables, the direction is determined by the scale. The variable with the wider scale will have more transfer change in the marginal and less change in the conditional.
    \item For equal scale, the direction is determined by the additive noise. With additive noise on Y, x2y is less likely than y2x, resulting in higher regret and better final fit of the conditional for y2x
    \item We assume that the root cause has the same variance as the additive components after. Does that really make sense?
    \item There is this spike in the middle of the parable. So gumbel softmax solves some problems of the network compensating (as they promise) but it does introduce some others.
    \item There is really no reason to believe there is such a thing as a root cause. The root cause distributional assumption is killing us :/
    \item Should we standardize everything that hasn't been intervened upon jointly? This standardization stuff is killing me. Why does it even work the way it is??
    \item Should I use the standardization as proposed by me? or should I use a separate network for every dataset?
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\clearpage
\subsection{Explanation}
\begin{itemize}
    \item x2y marginal fades out both ways, making it an easy fit for GMM. y2x marginal has one steep cutoff and a flat one, making it harder to fit.
    \item x2y conditional is hard to fit, for each x mdn needs to get it exactly right. y2x conditional is easier to fit, at least for some time the two strands are pretty close and there is a margin of error.
    \item This effect disappears once we use much narrower span values for transfer than we do for data generation
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[scale=0.8]{../src/transfer/experiments/v_equal/0transfer.png}
        \caption{No symmetry even for equal additive noise on X and Y}
    \end{subfigure}
    \hfill
    \caption{Bias visible in v-structure}
\end{figure}

\clearpage
\subsection{Application to Sachs}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKC_PRAF/data_causal.png}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKC_PRAF/comparison.png}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKA_PRAF/data_causal.png}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKA_PRAF/comparison.png}
    \end{subfigure}
\end{figure}


\subsection{Assumptions Bivariate DCDI}

Fundamentally there are two possible approaches to causal discovery using score-based models. We can either use a directional bias that allows for causal discovery even in the observational case, or we aim for score equivalence and make use of interventional data to distinguish between score equivalent models.

\paragraph{Marginal Distribution}
For any completely defined joint distribution we need to model the marginal distibution of root causes. Without a completely defined joint distribution, we can not reliably achieve score equivalence. However, once we specify the marginal, we can still only achieve score equivalence given the modeling assumptions are met. Score equivalence is a desirable property as it ensures that there is no bias toward picking a DAG from the MEC. We want only interventions, not any other biases to guide us to the DAG within the MEC. This intervention-only approach is motivated by the idea that we lack a justification for any other kind of directional bias. For linear models with standardized data, this is less of a problem.

\paragraph{Transformation}
Directional inference also relies on a model's ability to fit transformations similarly well in both directions. Again, the linear case is somewhat special here. If we have a constrained model, this will introduce a directional bias. With a sufficiently flexible model, we may be able to fit both directions equally well.

\paragraph{$D_{obs}$ and $D_{int}$}
Have to match our modeling assumptions. Notably, choosing an unsuitable interventional distribution can result in model misspecification.

\paragraph{Number of Obs and Int Samples} can also be decisive for directional inference.

\section{Formal Description}

Let data be given as \\
\begin{math}
    \begin{pmatrix} 
        a_{obs}^{(1)}\\
        \dots\\
        a_{obs}^{(n)}\\
    \end{pmatrix}
    % 
    \quad\overset{f(x)}{\to}\quad
    %
    \begin{pmatrix} 
        b_{obs}^{(1)}\\
        \dots\\
        b_{obs}^{(n)}\\
    \end{pmatrix}
    \\
    \textcolor{gray}{
        \begin{pmatrix} 
            a_{int}^{(1)}\\
            \dots\\
            a_{int}^{(m)}\\
        \end{pmatrix}
    }
    % 
    \quad\overset{f(x)}{\to}\quad
    % 
    \begin{pmatrix} 
        b_{int}^{(1)}\\
        \dots\\
        b_{int}^{(m)}\\
    \end{pmatrix}\\
\end{math}

\noindent
In this model, any $b = f(a) + N_b$ for corresponding $a$ and $b$. We observe the distributions $\mathcal{D}_{marg}^{a}, \mathcal{D}_{cond}^{b}, \mathcal{D}_{marg}^{a}, \mathcal{D}_{cond}^{b}$. In our causal model, we approximate $f(a)$ with $\widehat{f}(a)$, and any $\mathcal{D}$ with $\widehat{\mathcal{D}}$. With sample log-likelihoods $\mathbb{L}$ we can write the model score $\mathcal{S}$ as follows.\\

\begin{math}
    \mathcal{S}_{A \to B} = \mathbb{L}_{\widehat{\mathcal{D}}_{marg}}(a_{obs}) + 
                  \mathbb{L}_{\widehat{\mathcal{D}}_{cond}}(\widehat{f}(a_{obs})) +
                  \mathbb{L}_{\widehat{\mathcal{D}}_{cond}}(\widehat{f}(a_{int}))\\
\end{math}

\begin{math}
    \mathcal{S}_{B \to A} = \mathbb{L}_{\widehat{\mathcal{D}}_{cond}}(\widehat{f}^{-1}(b_{obs})) + 
                  \mathbb{L}_{\widehat{\mathcal{D}}_{marg}}(b_{obs}) +
                  \mathbb{L}_{\widehat{\mathcal{D}}_{marg}}(b_{int})\\
\end{math}

\noindent
\textbf{In the observational case} the direction depends entirely on the choice of marginal distribution, conditional distibution, and approximation of the transformation function.\\
\textbf{In the interventional case} a choice between the models amounts to a comparison of estimating transformation and condition compared to estimating only a marginal.


\section{Testing modelling assumptions}
The following tests can be performed on real world data to assess whether a causal discovery method is well specified for the task. Assessing score equivalence requires correct specification of marginal densities, conditional densities, and the ability to fit transformations equally well in both directions. Score equivalence can not tested directly, as we do not know the true distributions or causal structure. However, it's components can be tested individually.

\paragraph{Assumptions about marginal distributions} can be tested on the data. Each causal model will have to have at least one root cause, so there has to be at least one variable with a distribution matching the specification of the causal marginal distribution.

\paragraph{Assumptions about transformations and conditional distributions} are entangled and can therefore not be tested directly. However, we know that any two-variable pair in the observational part of the data should be score-equivalent, which we can test for.

\paragraph{Consistency in the presence of interventions} can also be tested. We propose a comparison of causal discovery results on different chunks of the data that are by themselves identifiable and should lead to the same causal model. One such example are two variables for which we have observational data and interventional data for each variable.

\subsection{When can we expect score equivalence?}
\begin{table}[H]
    \centering
    \begin{tabular}{|l|l|}
        \textbf{Identifiable} & \textbf{Non-identifiabe}\\
        \hline
        SE void & Check SE\\
    \end{tabular}
\end{table}


\subsection{Checking Model Assumptions}
Can there be a case where interventional data flips the direction of causal inference? (as seen on the experimental data)

Within these noise models, we distinguish identifiable and non-identifiable ones as $I$ and $\bar{I}$.
For now we restrict ourselves to generic gaussian noise, but the argument can be extended to other distributions.
Within the gaussian noise class, we distinguish between the linear kind (LANM) and the non-linear kind (NAMN).
LAMNs are generally not identifiable, unless special conditions about the (conditional) variances hold \todo{cite Peters, Park, Reisach} and are built into the model. 
NAMNs are identifiable by a joint gaussian model which can fit the class of nonlinear functions.
Since the identifiability assumptions for LANMs are difficult to test, we assume a score that does not take them into account.
For such a score, we can denote the set of all additive noise models as $C = (LAMN \in \bar{I}, NAMN \in I)$.

We now consider the components to a causal graphical model which is a triple of (Class of marginal distributions, Class of conditional distributions, Class of transformations) formalized as $(\mathbb{D}_{marg}, \mathbb{D}_{cond}, \mathbb{F})$.

A suitable score function should assign a an equal score for markov equivalent graphs.


In a typical graphical model, the variables are sums of transformed values of their parent variables. For a structure learning algorithm to be able to recover the data generating graphical model, we thus need it to be able to fit the expression

\begin{equation}
    \widehat{\mathcal{D}}_{cond}(\sum_{p=1}^{n\_parents}\widehat{\mathcal{F}}(x^{(p)}), \sigma).
    \label{eq:score}
\end{equation}

for arbitrary $\mathcal{D}_{marg} \in \mathbb{D}_{marg}, \mathcal{D}_{cond} \in \mathbb{D}_{cond}, \mathcal{F} \in \mathbb{F}$.
In doing so, we assume that the summation of variables does not change their distribution, which implis that that $\mathbb{D}_{marg} = \mathbb{D}_{cond}$\todo{does it?}.
Consider the following causal graphical model.
\begin{figure}[H]
    \centerline{\includegraphics[height=8.7em]{figures/confounder.png}}
\end{figure}
We let confounder $C$ model any common confounders, $P_A, P_B$ are any unknown parents of $A$ and $B$ respectively, and $M$ captures an unknown number of potential mediators. This generic scenario captures the choice of any two variables in a causal graph. We can see that again, each of A and B will be the sum of (potentially chained) transformations of parent variables. Crucially, the fact that some of these parents overlap should not impede our ability to fit the distribution \todo{can I argue this?}. Thus if we assume that our score can fit a generic variable of the form specified in \cref{eq:score}, it is also able to correcly fit any subgraph consisting of two nodes $A$ and $B$.

% We can now write the densities of $A$ and $B$ as follows.
% \begin{equation}
%     \widehat{\mathcal{D}}_{cond}^A(\sum_{p=1}^{n\_parents}\widehat{\mathcal{F}}(x^{(p)}), \sigma) \\
% \end{equation}


\section{Code}
\lstinputlisting{test.py}


\clearpage
\printbibliography

\end{document}