\documentclass{article}

\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref}
\usepackage[at]{easylist}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage[
    doi=false,isbn=false,url=false,
    backend=biber,
    uniquename=false,
    bibencoding=utf8,
    sorting=nty,
    citestyle=authoryear,
    ]{biblatex}
    \addbibresource{bibliography.bib}

\title{Reading Notes}
\author{Alexander Reisach}

\begin{document}
    
\maketitle

\section{Plan}
\begin{easylist}
    \ListProperties(Style1*=\bfseries,Numbers2=l,Mark1={},Mark2={)},Indent2=1em)
    @ Start by looking at data generation methods
    @@ Can I find the \cite{sachs2005causal} pathways in \cite{belinky2015pathcards} and \cite{perfetto2016signor}?
    @@ Do \cite{van2006syntren} and \cite{schaffter2011genenetweaver} produce varsortable data?
    @ Take a look at real-world data
    @ Implement \cite{brouillard2020differentiable}
    @ Consider transfer learning regret
    @ Try cycle breaking
\end{easylist}

\section{Related Literature}
\cite{itani2010structure} provide a good recipe for learning cyclical causal structure from interventional data.
\cite{vowels2021} give a comprehensive overview over all kinds of structure learning methods and benchmarks.
\cite{van2006syntren} provide a good overview over other network simulations.

\subsection{Datasets}

I am not sure there are much better data sets than the one by \cite{sachs2005causal}.
In the paper they talk about some well-known connections. Take those and look at them!
Several of the known connections from
our model are direct enzyme-substrate rela-
tionships (Fig. 3B) (PKA to Raf, Raf to Mek,
Mek to Erk, and Plc-g to PIP 2 ),

\paragraph{SynTReN}
is a synthetic TRN data generation scheme proposed by \cite{van2006syntren}. 
They talk about ODE simulations. Should have a look at them.
Check out the original source networks they consider, also look for more up-to-date networks
They seem to just do additive noise models
Visualize and compare that to Sachs et al and another dataset from vowels2021

\paragraph{GeneNetWeaver} is a synthetic TRN data generation scheme proposed by \cite{schaffter2011genenetweaver}. Their model was used in benchmarking the Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenges.

\paragraph{SIGNOR} is a database of established causal relationships between biological entities \cite{perfetto2016signor}.

\paragraph{PathCards} unifies several sources on human biological pathways \cite{belinky2015pathcards}.
They provide a good overview over other pathway sources.

\subsection{Benchmarking}

Ernest Fraenkel\footnote{\url{https://www.youtube.com/watch?v=RBPcKbEvK3U&list=PLUl4u3cNGP63uK-oWiLgO7LLJV6ZCWXac&index=17}} gives a very nice overview over \cite{marbach2012wisdom} where he explains how some funamental assumptions may be violated in real-world cell signalling data.
He shows that unlike in in-silico data and E.coli data, for yeast co-regulated genes are basically uncorrelated and have no mutual information.

Tuebingen causality pairs \url{https://webdav.tuebingen.mpg.de/cause-effect/} (only observational).
Try varsortability in the bivariate case?

\section{Conceptional Work}
    - Leap of abstraction from PDE to causal graph

\section{Methodology}
\begin{itemize}
    \item Only apply acyclicity constraint gradually/at a later stage?
    \item Do not optimize over diagonal?
    \item Use dropout (predictions/gradients)?
    \item Combine continuous structure learning with cyclic relationships
\end{itemize}

\subsection{Transfer Learning Regret}
Do \cite{bengio2019meta} normalize? if not, isn't maybe the variance in one direction just bigger than in the other which requires more steps? In the high-variance direction, we might see bigger losses at the start $\dots$
How do they generate their data?

In Bengio's paper, are we ultimately just talking about inverting a (mathematically) irreversible function?
Isn't that conceptionally exactly the same as the light switch example where X inevitably causes Y but Y can also be caused by many other things?

Is there something to be gained from viewing data as a sequence in the style of \cite{bengio2019meta}? Mybe try transfer learning regret on bio data?

Is it maybe enough to give the conditional more params to fit than the marginal? :O
Actually, can we just test which way the marginal changes more? The conditional will have to account for the rest of the variation...

Is there a way to break causality by including the wrong things? Not just by treating them in the wrong way, but by including them in the question at all!

\subsection{Experiments}
\paragraph{Current theory}
Whichever model has the higher nll, that's the model the meta-learner will move towards?
Alternatively, maybe meta learning will work once we have output noise??
We do have a lot of random fluctuations either way...
With output noise, the conditional loss of y2x seems to be a lot lower. Not anymore without output noise?
\begin{itemize}
    \item TODO maybe there is catastrophic forgetting in only one direction? say, if in one direction it would be easier to re-fit the already fitted components / refitting would require some lasting change!
    \item Does standardization remove impact of scale range?
    \item output noise has an effect on the relative data scales! With sufficiently small additive noise, we can get alpha to go either way
    \item Is it about which part of the network would have to 'move the farthest'??
    \item The network learns combinations that work fine for the input range but go bananas beyound, even for simple stuff like all zeros, or abline targets
    \item One could look at 'samples until convergence', rather than total regret, that would effectively check how much params have to change.
    \item I am still not quite sure if its a bias of their model or something about the data generation
\end{itemize}

\clearpage
\subsection{Findings}
\begin{itemize}
    \item We need to add some significant noise to at least one variable, otherwise we risk collapsing probabilities
    \item Evaluation data set needs to be large for consistent effects
    \item For equal additive noise on both variables, the direction is determined by the scale. The variable with the wider scale will have more transfer change in the marginal and less change in the conditional.
    \item For equal scale, the direction is determined by the additive noise. With additive noise on Y, x2y is less likely than y2x, resulting in higher regret and better final fit of the conditional for y2x
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\clearpage
\subsection{Explanation}
\begin{itemize}
    \item x2y marginal fades out both ways, making it an easy fit for gmm. y2x marginal has one steep cutoff and a flat one, making it harder to fit.
    \item x2y conditional is hard to fit, for each x mdn needs to get it exactly right. y2x conditional is easier to fit, at least for some time the two strands are pretty close and there is a margin of error.
    \item This effect disappears once we use much narrower span values for transfer than we do for data generation
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[scale=0.8]{../src/transfer/experiments/v_equal/0transfer.png}
        \caption{No symmetry even for equal additive noise on X and Y}
    \end{subfigure}
    \hfill
    \caption{Bias visible in v-structure}
\end{figure}

\clearpage
\printbibliography

\end{document}