\documentclass{article}

\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage[at]{easylist}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage[
    doi=false,isbn=false,url=false,
    backend=biber,
    uniquename=false,
    bibencoding=utf8,
    sorting=nty,
    citestyle=authoryear,
    ]{biblatex}
    \addbibresource{bibliography.bib}

\title{Reading Notes}
\author{Alexander Reisach}

\begin{document}
    
\maketitle

\section{Plan}
\begin{easylist}
    \ListProperties(Style1*=\bfseries,Numbers2=l,Mark1={},Mark2={)},Indent2=1em)
    @ Start by looking at data generation methods
    @@ Can I find the \cite{sachs2005causal} pathways in \cite{belinky2015pathcards} and \cite{perfetto2016signor}?
    @@ Do \cite{van2006syntren} and \cite{schaffter2011genenetweaver} produce varsortable data?
    @ Take a look at real-world data
    @ Implement \cite{brouillard2020differentiable}
    @ Consider transfer learning regret
    @ Try cycle breaking
\end{easylist}

\section{Related Literature}
Nice resource on cell signalling pathways: \url{https://www.khanacademy.org/science/ap-biology/cell-communication-and-cell-cycle/changes-in-signal-transduction-pathways/a/intracellular-signal-transduction}.

\subsection{Causal Structure Learning}
\cite{itani2010structure} provide a good recipe for learning cyclical causal structure from interventional data.
\cite{vowels2021} give a comprehensive overview over all kinds of structure learning methods and benchmarks.
\cite{van2006syntren} provide a good overview over other network simulations.

\subsubsection{DCDI}
\noindent
TODOs
\begin{enumerate}
    \item Shouldnt they have to copy x at some point in their loss calculation?
    \item What exactly are their "conditional distributions"?
    \item Check out their "plot-density" for the two-variable case!
    \item How does their obs data solution compare to lars lasso bic regression with random direction? 
    \item How does this relate to the interventional case?
    \item Can I create a "counterexample" with some special interventions?
    \item What value do obs data give us? Is there an optimal ratio obs/int 
    data?
    \item What's the impact of interventions having a different distribution?
    \item don't a lot of things just depend on our number of (obs/int) data points? (which will determine our likelihood)
    \item What are these extra params?
    \item Check the different gumbel matrices
    \item We should try treating perfect inter as unknown but with identical params (mask out at input stage)
    \item Their method (with standardization per regime) should not work for non-linear relationships
    \item Where do the spikes come from in the loss chart? Are we losing the NLL-params at every aug. lagrangian update step?
    \item We'd have to have a network with optimal parameters ready for every possible combinations of inputs? Since that's not feasible, can the networks retrain sensibly?
    \item TODO I should look at that paper that claims to have 70\% success on the tuebingen cause-effect data set.
\end{enumerate}

\noindent
Observations
\begin{enumerate}
    \item They are scaling each interventional distribution separately right after generation (dat\_generator.py). This might lose a lot of information? In their default settings they use uniform, so it is less severe. But for shift\_normal, it counteracts some of the purpose of the intervention.
    \item In their data they seem to have standardized per regime
    \item They also standardize the entire dataset later, but those parts of the code are bugged and do not work (generate\_data.py)!
    \item Getting the combination of lr and lagrangian penalties right is not straightforward. We need to keep the mu increases small.
    \item In their implementation, they effectively model distributions first, then slowly let the lagrangian kick in
    \item They do not solve the observational case particularly well (they get the causal direction in Gaussian NV ANMs 50-50, just as one would expect and like notears.)
    \item Outlier removal might make a difference, at least in the MEK-ERK case.
    \item MEK-ERK works; PKA-P38 fails; PKA-JNK works; PKA-ERK works;
    \item It seems that too large a body of obs data can render the int data ineffective
    \item Stochasticity at small batch size can be a problem
    \item What are they doing with name\_data??? It's never read at all? Currently we are applying intervention masks to observational data?
    \item Their sparsity constraint hyperparam can really mess things up. Is there a good way of setting it? By setting it to zero, I can learn something sensible in the bivariate case
    \item For MEK the outliers cause a completely different distributin after scaling in the obs setting
    \item Whether or not to standardize regimes individually is not that straightforward!
    \item U0126 perturbation might be a special case as it does not prevent phosphorylation but inhibits otherwise from what I understand. We see this across charts, e.g. in RAF-MEK - seems like a lot more MEK is produced under inhibitor. Interestingly, under U0126, there is a clear trend between PKC and JNK that's otherwise absent.
    \item Their compute\_loss nll still doesn't seem right. Shouldn't they divide by the number of non-zero mask entries?
    \item Distribution fits do not get worse because mask is applied only at loss calculation.
    \item If I am right about the potential but in nll calculation, then I should see that an intervention on the effect inevitably leats to learning the effect as a cause!
    \item Why is their code working even if I intervene on effect???
    \item Sometimes their nlls are negative. Can that be a problem? logsumexp?
    \item Maybe I should standardize each intervention separately??? the shift from zero might be why we learn the correct causal direction, even with an intervention on the effect node!
    \item We have to scale each regime individually for likelihood, but doesn't that also loose some (a lot of) information? is there a logical contradiction?
    \item For the two variables I picked, which have a pretty clear linear trend, if I mask out 50\% such that they can only be used for explanations one way - it should kind of work. I wonder if  there is a way to connect this back to the bengio transfer-learning paper. After all DCDI also chooses the model that minimizes conditional nll. If the optimal likelihoods X->Y and Y<-X are quite similar, perhaps the intervention will make the difference. If they are quite different, we might need a very strong intervention to overcome that inherent bias...
    \item Seems like whether or not the trend in int and obs is the same makes quite a difference (at least when the support overlaps?). This might be related to the obs/marg likelihoods.
    \item I'm still not quite sure about the standardization procedures in that regard. I've been thinking that one should standardize the entire data set rather than each regime. That would allow to take advantage of potential increases in support through intervantions. On the flip sice, in the case of a shift intervention, it really compresses both peaks which has an impact on the likelihood.
    In DCDI's initial implementation where they standardize each regime separately, that is not a problem. However, they may end up with having the same support for observational and interventional data and will require the optimal weight to be identical, even if it was rightfully different on the raw scale.
    \item extra params model log stds (learnable but do not depend on parents) That means distribution stds do not depend on parents?
    \item If not one of two opposing edges goes to zero immediately, they will 'fight' and both end up at zero. There needs to be a convincing winner. The problem could potentially be mitigated by choosing smaller mu update steps in the lagrangian.
    \item The acyclicity constraint may eliminate the inputs to cycles before the actual cycles. This could be a fundamental problem!
    \item I am still very unsure about the individual standardization
    \item They assume homoscedasticity
    \item The adversarial sachs et al example should also be bad news for sparsity constraints, no?
    \item Under model misspecification I have no score equivalence either way. Why no take score equivalence to make sure I specified my model correcty?
\end{enumerate}

\subsection{Score}

% \begin{equation}
    
% \end{equation}

\subsubsection{General Model}
\begin{equation}
    f^{(k)}(X, M, R, \phi) = \prod_{j=1}^{d} \tilde{f}(x_j, NN(M_j \odot x, \phi^{(1)}))^{1-R_{kj}} \tilde{f}(x_j, NN(M_j \odot x, \phi^{(k)}))^{R_{kj}}
\end{equation}

\begin{equation}
    S_{\mathcal{I}}(G) = \sup \sum_{k=1}^K \mathbb{E}_{X ~ p^{k}} \log f^{(k)}(X, M, R, \phi) - \lambda |G|
\end{equation}

% TODO: check if mean is in right place
\begin{equation}
    \underset{G}{\mathrm{argmax}} \quad \frac{1}{K} \sum_{k=1}^K \frac{1}{N} \sum_{i=1}^N \log(\tilde{f}(x_j, NN(M_j \odot x, \phi^{(1)}))^{1-R_{kj}} \tilde{f}(x_j, NN(M_j \odot x, \phi^{(k)}))^{R_{kj}}) - \lambda |G|
\end{equation}

\subsubsection{Perfect Interventions}
% Score for perfect interventions
\begin{equation}
    \begin{split}
        & X \in \mathbb{R}^{n \times d} \\
        & M^G \in \mathbb{R}^{d \times d} := \text{sigmoid}(G) > 0.5 \\
        & \mathcal{M} \in \mathbb{R}^{n \times d} \quad \text{(intervention mask)} \\
        & \underset{G}{\mathrm{argmax}} \quad \sum^n_{i=1} \sum^d_{j=1} \mathcal{N}(\text{NN}(M^G_j \odot x^{(i)}, \phi), 1)_{\text{logPDF}}(x^{(i)}_j) \cdot \mathcal{M}^{(i)}_j
    \end{split}
\end{equation}


\subsection{Pseudocode}
\begin{verbatim}
    for i in range(num_vars):
        
        if intervention == 'perfect':
            density_params = model(x, weights, biases, extra_params)
            log_probs = distribution(density_params).log_prob(x[:, i]) * mask

        else:  # imperfect/unknown intervention
            density_params = model(x, weights, biases, extra_params, masks, regime)
            log_probs = distribution(density_params).log_prob(x[:. i])
        
        log_likelihood = torch.mean(log_probs)

\end{verbatim}


\subsection{Causal Networks in Biology}
The original paper on the dream challenges etc. is \cite{hill2016inferring}.
DREAM challenge data should be available here \url{https://www.synapse.org/#!Synapse:syn1720047}.
\cite{hill2017context} address the problem of benchmarking causal network inference in biological data. Seems like they just recycled some of their findings from their 2016 paper? They say: However, we note that empirical
assessment is a frontier topic in causal inference, and the
assessment procedure used here is subject to a number of ca-
veats.
The \emph{in silico} data used in the DREAM challenges was generated according to \cite{chen2009input}.
They are using some sort of funny score and don't seem to trust it a lot themselves. Previous DREAM challenges were predictive rather than causal.
For the assessment they use some unseen interventions to get a gold standard network and du AUROC on that.

\paragraph{Info on DREAM data}
Basic information on the dataset \url{https://www.synapse.org/#!Wiki:syn1720047/ENTITY/56061}. Additional information on the dataset\url{https://www.synapse.org/#!Wiki:syn1720047/ENTITY/56210}. The webpage for the 2013 DREAM challenge can be found here \url{https://dreamchallenges.org/dream-8-hpn-dream-breast-cancer-network-inference-challenge/}.

Nice overview on cell network inference here \url{http://compbio.mit.edu/marbach/projectsdaniel.html}.

This 5-node network in yeast might also be useful \cite{cantone2009yeast}.

\vspace{.5em}

It looks like this guz\url{https://github.com/gungorbudak/netinf-bigcat} did a pretty similar thing at bigcat in Maastricht 8 years ago?
This MSc thesis is also very similar in spirit \url{https://github.com/ninakersten/Masterthesis}.

\subsection{Datasets}

I am not sure there are much better data sets than the one by \cite{sachs2005causal}.
In the paper they talk about some well-known connections. Take those and look at them!
Several of the known connections from
our model are direct enzyme-substrate rela-
tionships (Fig. 3B) (PKA to Raf, Raf to Mek,
Mek to Erk, and Plc-g to PIP 2 ),

\paragraph{SynTReN}
is a synthetic TRN data generation scheme proposed by \cite{van2006syntren}. 
They talk about ODE simulations. Should have a look at them.
Check out the original source networks they consider, also look for more up-to-date networks
They seem to just do additive noise models
Visualize and compare that to Sachs et al and another dataset from vowels2021

\paragraph{GeneNetWeaver} is a synthetic TRN data generation scheme proposed by \cite{schaffter2011genenetweaver}. Their model was used in benchmarking the Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenges.

\paragraph{SIGNOR} is a database of established causal relationships between biological entities \cite{perfetto2016signor}.

\paragraph{PathCards} unifies several sources on human biological pathways \cite{belinky2015pathcards}.
They provide a good overview over other pathway sources.

\paragraph{OmniPath} I should probably use OmniPath to make sure the neetworks used in the DREAM challenge are still up to date?

\subsection{Benchmarking}

Ernest Fraenkel\footnote{\url{https://www.youtube.com/watch?v=RBPcKbEvK3U&list=PLUl4u3cNGP63uK-oWiLgO7LLJV6ZCWXac&index=17}} gives a very nice overview over \cite{marbach2012wisdom} where he explains how some funamental assumptions may be violated in real-world cell signalling data.
He shows that unlike in in-silico data and E.coli data, for yeast co-regulated genes are basically uncorrelated and have no mutual information.

Tuebingen causality pairs \url{https://webdav.tuebingen.mpg.de/cause-effect/} (only observational).
Try varsortability in the bivariate case?


\section{Conceptional Work}
    - Leap of abstraction from PDE to causal graph

\section{Methodology}
\begin{itemize}
    \item Only apply acyclicity constraint gradually/at a later stage?
    \item Do not optimize over diagonal?
    \item Use dropout (predictions/gradients)?
    \item Combine continuous structure learning with cyclic relationships
\end{itemize}

\subsection{Transfer Learning Regret}
Do \cite{bengio2019meta} normalize? if not, isn't maybe the variance in one direction just bigger than in the other which requires more steps? In the high-variance direction, we might see bigger losses at the start $\dots$
How do they generate their data?

In Bengio's paper, are we ultimately just talking about inverting a (mathematically) irreversible function?
Isn't that conceptionally exactly the same as the light switch example where X inevitably causes Y but Y can also be caused by many other things?

Is there something to be gained from viewing data as a sequence in the style of \cite{bengio2019meta}? Mybe try transfer learning regret on bio data?

Is it maybe enough to give the conditional more params to fit than the marginal? :O
Actually, can we just test which way the marginal changes more? The conditional will have to account for the rest of the variation...

Is there a way to break causality by including the wrong things? Not just by treating them in the wrong way, but by including them in the question at all!

\subsection{Experiments}
\paragraph{Current theory}
Whichever model has the higher nll, that's the model the meta-learner will move towards?
Alternatively, maybe meta learning will work once we have output noise??
We do have a lot of random fluctuations either way...
With output noise, the conditional loss of y2x seems to be a lot lower. Not anymore without output noise?
\begin{itemize}
    \item TODO maybe there is catastrophic forgetting in only one direction? say, if in one direction it would be easier to re-fit the already fitted components / refitting would require some lasting change!
    \item Does standardization remove impact of scale range?
    \item output noise has an effect on the relative data scales! With sufficiently small additive noise, we can get alpha to go either way
    \item Is it about which part of the network would have to 'move the farthest'??
    \item The network learns combinations that work fine for the input range but go bananas beyound, even for simple stuff like all zeros, or abline targets
    \item One could look at 'samples until convergence', rather than total regret, that would effectively check how much params have to change.
    \item I am still not quite sure if its a bias of their model or something about the data generation
\end{itemize}

\clearpage
\subsection{Findings}
\begin{itemize}
    \item We need to add some significant noise to at least one variable, otherwise we risk collapsing probabilities
    \item Evaluation data set needs to be large for consistent effects
    \item For equal additive noise on both variables, the direction is determined by the scale. The variable with the wider scale will have more transfer change in the marginal and less change in the conditional.
    \item For equal scale, the direction is determined by the additive noise. With additive noise on Y, x2y is less likely than y2x, resulting in higher regret and better final fit of the conditional for y2x
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\clearpage
\subsection{Explanation}
\begin{itemize}
    \item x2y marginal fades out both ways, making it an easy fit for GMM. y2x marginal has one steep cutoff and a flat one, making it harder to fit.
    \item x2y conditional is hard to fit, for each x mdn needs to get it exactly right. y2x conditional is easier to fit, at least for some time the two strands are pretty close and there is a margin of error.
    \item This effect disappears once we use much narrower span values for transfer than we do for data generation
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[scale=0.8]{../src/transfer/experiments/v_equal/0transfer.png}
        \caption{No symmetry even for equal additive noise on X and Y}
    \end{subfigure}
    \hfill
    \caption{Bias visible in v-structure}
\end{figure}

\clearpage
\subsection{Application to Sachs}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKC_PRAF/data_causal.png}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKC_PRAF/comparison.png}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKA_PRAF/data_causal.png}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKA_PRAF/comparison.png}
    \end{subfigure}
\end{figure}


\clearpage
\printbibliography

\end{document}