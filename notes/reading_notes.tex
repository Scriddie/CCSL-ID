\documentclass{article}

\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{hyperref}
\usepackage[at]{easylist}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subcaption}
\usepackage[
    doi=false,isbn=false,url=false,
    backend=biber,
    uniquename=false,
    bibencoding=utf8,
    sorting=nty,
    citestyle=authoryear,
    ]{biblatex}
    \addbibresource{bibliography.bib}

\title{Reading Notes}
\author{Alexander Reisach}

\begin{document}
    
\maketitle

\section{Plan}
\begin{easylist}
    \ListProperties(Style1*=\bfseries,Numbers2=l,Mark1={},Mark2={)},Indent2=1em)
    @ Start by looking at data generation methods
    @@ Can I find the \cite{sachs2005causal} pathways in \cite{belinky2015pathcards} and \cite{perfetto2016signor}?
    @@ Do \cite{van2006syntren} and \cite{schaffter2011genenetweaver} produce varsortable data?
    @ Take a look at real-world data
    @ Implement \cite{brouillard2020differentiable}
    @ Consider transfer learning regret
    @ Try cycle breaking
\end{easylist}

\section{Related Literature}
Nice resource on cell signalling pathways: \url{https://www.khanacademy.org/science/ap-biology/cell-communication-and-cell-cycle/changes-in-signal-transduction-pathways/a/intracellular-signal-transduction}.

\subsection{Causal Structure Learning}
\cite{itani2010structure} provide a good recipe for learning cyclical causal structure from interventional data.
\cite{vowels2021} give a comprehensive overview over all kinds of structure learning methods and benchmarks.
\cite{van2006syntren} provide a good overview over other network simulations.

\subsubsection{DCDI}
\begin{enumerate}
    \item Why does the adjacency matrix not change???
    \item Go through "forward_given_params" in base_model
    \item How do they generate data? Check \cite{hauser2012characterization}
    \item What are their data generation regimes???
    \item Shouldnt they have to copy x at some point in their loss calculation?
    \item What exactly are their "conditional distributions"?
    \item Check out their "plot-density" for the two-variable case!
    \item Where exactly does the adj mat learning come in?
    \item How (well) do they solve the observational case?
    \item How does this relate to the interventional case?
    \item Can I create a "counterexample" with some special interventions?
\end{enumerate}

\noindent
Observations
\begin{enumerate}
    \item Getting the combination of lr and lagrangian penalties right is not straightforward
    \item In their implementation, they effectively model distributions first, then slowly let the lagrangian kick in
\end{enumerate}

\paragraph{Where does the adjacency matrix come in?}
In einsum notation of BaseModel forward

\paragraph{How do they evaluate fit?}
NLL of their generative model

Random Notes
\begin{itemize}
    \item They normalize their data
\end{itemize}

\subsection{Causal Networks in Biology}
The original paper on the dream challenges etc. is \cite{hill2016inferring}.
DREAM challenge data should be available here \url{https://www.synapse.org/#!Synapse:syn1720047}.
\cite{hill2017context} address the problem of benchmarking causal network inference in biological data. Seems like they just recycled some of their findings from their 2016 paper? They say: However, we note that empirical
assessment is a frontier topic in causal inference, and the
assessment procedure used here is subject to a number of ca-
veats.
The \emph{in silico} data used in the DREAM challenges was generated according to \cite{chen2009input}.
They are using some sort of funny score and don't seem to trust it a lot themselves. Previous DREAM challenges were predictive rather than causal.
For the assessment they use some unseen interventions to get a gold standard network and du AUROC on that.

\paragraph{Info on DREAM data}
Basic information on the dataset \url{https://www.synapse.org/#!Wiki:syn1720047/ENTITY/56061}. Additional information on the dataset\url{https://www.synapse.org/#!Wiki:syn1720047/ENTITY/56210}. The webpage for the 2013 DREAM challenge can be found here \url{https://dreamchallenges.org/dream-8-hpn-dream-breast-cancer-network-inference-challenge/}.

Nice overview on cell network inference here \url{http://compbio.mit.edu/marbach/projectsdaniel.html}.

This 5-node network in yeast might also be useful \cite{cantone2009yeast}.

\vspace{.5em}

It looks like this guz\url{https://github.com/gungorbudak/netinf-bigcat} did a pretty similar thing at bigcat in Maastricht 8 years ago?
This MSc thesis is also very similar in spirit \url{https://github.com/ninakersten/Masterthesis}.

\subsection{Datasets}

I am not sure there are much better data sets than the one by \cite{sachs2005causal}.
In the paper they talk about some well-known connections. Take those and look at them!
Several of the known connections from
our model are direct enzyme-substrate rela-
tionships (Fig. 3B) (PKA to Raf, Raf to Mek,
Mek to Erk, and Plc-g to PIP 2 ),

\paragraph{SynTReN}
is a synthetic TRN data generation scheme proposed by \cite{van2006syntren}. 
They talk about ODE simulations. Should have a look at them.
Check out the original source networks they consider, also look for more up-to-date networks
They seem to just do additive noise models
Visualize and compare that to Sachs et al and another dataset from vowels2021

\paragraph{GeneNetWeaver} is a synthetic TRN data generation scheme proposed by \cite{schaffter2011genenetweaver}. Their model was used in benchmarking the Dialogue for Reverse Engineering Assessments and Methods (DREAM) challenges.

\paragraph{SIGNOR} is a database of established causal relationships between biological entities \cite{perfetto2016signor}.

\paragraph{PathCards} unifies several sources on human biological pathways \cite{belinky2015pathcards}.
They provide a good overview over other pathway sources.

\paragraph{OmniPath} I should probably use OmniPath to make sure the neetworks used in the DREAM challenge are still up to date?

\subsection{Benchmarking}

Ernest Fraenkel\footnote{\url{https://www.youtube.com/watch?v=RBPcKbEvK3U&list=PLUl4u3cNGP63uK-oWiLgO7LLJV6ZCWXac&index=17}} gives a very nice overview over \cite{marbach2012wisdom} where he explains how some funamental assumptions may be violated in real-world cell signalling data.
He shows that unlike in in-silico data and E.coli data, for yeast co-regulated genes are basically uncorrelated and have no mutual information.

Tuebingen causality pairs \url{https://webdav.tuebingen.mpg.de/cause-effect/} (only observational).
Try varsortability in the bivariate case?


\section{Conceptional Work}
    - Leap of abstraction from PDE to causal graph

\section{Methodology}
\begin{itemize}
    \item Only apply acyclicity constraint gradually/at a later stage?
    \item Do not optimize over diagonal?
    \item Use dropout (predictions/gradients)?
    \item Combine continuous structure learning with cyclic relationships
\end{itemize}

\subsection{Transfer Learning Regret}
Do \cite{bengio2019meta} normalize? if not, isn't maybe the variance in one direction just bigger than in the other which requires more steps? In the high-variance direction, we might see bigger losses at the start $\dots$
How do they generate their data?

In Bengio's paper, are we ultimately just talking about inverting a (mathematically) irreversible function?
Isn't that conceptionally exactly the same as the light switch example where X inevitably causes Y but Y can also be caused by many other things?

Is there something to be gained from viewing data as a sequence in the style of \cite{bengio2019meta}? Mybe try transfer learning regret on bio data?

Is it maybe enough to give the conditional more params to fit than the marginal? :O
Actually, can we just test which way the marginal changes more? The conditional will have to account for the rest of the variation...

Is there a way to break causality by including the wrong things? Not just by treating them in the wrong way, but by including them in the question at all!

\subsection{Experiments}
\paragraph{Current theory}
Whichever model has the higher nll, that's the model the meta-learner will move towards?
Alternatively, maybe meta learning will work once we have output noise??
We do have a lot of random fluctuations either way...
With output noise, the conditional loss of y2x seems to be a lot lower. Not anymore without output noise?
\begin{itemize}
    \item TODO maybe there is catastrophic forgetting in only one direction? say, if in one direction it would be easier to re-fit the already fitted components / refitting would require some lasting change!
    \item Does standardization remove impact of scale range?
    \item output noise has an effect on the relative data scales! With sufficiently small additive noise, we can get alpha to go either way
    \item Is it about which part of the network would have to 'move the farthest'??
    \item The network learns combinations that work fine for the input range but go bananas beyound, even for simple stuff like all zeros, or abline targets
    \item One could look at 'samples until convergence', rather than total regret, that would effectively check how much params have to change.
    \item I am still not quite sure if its a bias of their model or something about the data generation
\end{itemize}

\clearpage
\subsection{Findings}
\begin{itemize}
    \item We need to add some significant noise to at least one variable, otherwise we risk collapsing probabilities
    \item Evaluation data set needs to be large for consistent effects
    \item For equal additive noise on both variables, the direction is determined by the scale. The variable with the wider scale will have more transfer change in the marginal and less change in the conditional.
    \item For equal scale, the direction is determined by the additive noise. With additive noise on Y, x2y is less likely than y2x, resulting in higher regret and better final fit of the conditional for y2x
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \begin{subfigure}{.32\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_equal/0transfer.png}
        \caption{For abline and equal additive noise, transfer is symmetrical.}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_X/0transfer.png}
        \caption{Additive noise on X}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_noise_Y/0transfer.png}
        \caption{Additive noise on Y}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_X/0transfer.png}
        \caption{X on narrow scale}
    \end{subfigure}
    \hfill
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/theirs_squash_Y/0transfer.png}
        \caption{Y on narrow scale}
    \end{subfigure}
    \hfill
\end{figure}

\clearpage
\subsection{Explanation}
\begin{itemize}
    \item x2y marginal fades out both ways, making it an easy fit for GMM. y2x marginal has one steep cutoff and a flat one, making it harder to fit.
    \item x2y conditional is hard to fit, for each x mdn needs to get it exactly right. y2x conditional is easier to fit, at least for some time the two strands are pretty close and there is a margin of error.
    \item This effect disappears once we use much narrower span values for transfer than we do for data generation
\end{itemize}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/x2y_data.png}
        \caption{x2y data}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/v_equal/y2x_data.png}
        \caption{y2x data}
    \end{subfigure}
    \hfill
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[scale=0.8]{../src/transfer/experiments/v_equal/0transfer.png}
        \caption{No symmetry even for equal additive noise on X and Y}
    \end{subfigure}
    \hfill
    \caption{Bias visible in v-structure}
\end{figure}

\clearpage
\subsection{Application to Sachs}

\begin{figure}[H]
    \centering
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKC_PRAF/data_causal.png}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKC_PRAF/comparison.png}
    \end{subfigure}
    \hfill
    % 
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKA_PRAF/data_causal.png}
    \end{subfigure}
    \begin{subfigure}{.49\linewidth}
        \centering
        \includegraphics[width=\linewidth]{../src/transfer/experiments/PKA_PRAF/comparison.png}
    \end{subfigure}
\end{figure}


\clearpage
\printbibliography

\end{document}